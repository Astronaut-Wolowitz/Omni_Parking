{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khare, Ankit\n",
    "# 1001-367-474\n",
    "# 2017-04-17\n",
    "# Project_final_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Identifying empty parking spaces in a Car Parking</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "A system is proposed which classifies the parking space as occupied or vacant. Such a system is highly useful in scenarios where a camera is placed at a lamp post view and parking spaces are visible. The system uses a combination of Laplacian operator for edge detection, HAAR classifier for object recognition and motion tracking to distinguish between the parked and vacant spaces. Motion tracking using background subtraction techniques, contours and Morphological operations is used. Further, Pedestrian detection using HOG and SVM is incorporated to detect pedestrains in the parking region.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupied and Vacant Parking Space Classification Approach\n",
    "HAAR classifier was compared with SVM and HOG. Although HOG combined with SVM gives more prevision in comparison to using HAAR alone but the processing required for HAAR is way lesser as compared to HOG. As a result, HAAR was finalized to be implemented. HOG's pedestrian detection inbuilt opencv2 function is used but it can be observed by turning it on by how how it educes the speed of execution. For enhancing the accuracy of classification as a vacant or occupied space, Laplacian operator is used. A threshold is established where the possibility of the presence of vehicle is maximum. Basically, parking regions are laid out manually and a threshold is defined by calculating the magnitude of edges inside of them. A car will have some significant edges, while empty parking spot will be smooth. This is the fact behind the use of Laplacian technique. If the possibility of the presence of a vehicle is beyond threshold then the classifier is applied to detect if it is a vehicle or not. Whenever a change in threshold is found, classifier is called to check whther it's a vehicle or not. It it is found to be a vehicle then it is marked and it is observed for motion. Whenever there's a motion at the point, again the classifier is called to determine the presence of vehicle. Hence the system proposes a combination of detection, tracking and classification to achieve efficient parking.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results\n",
    "A utility program was used to calculate the accuracy of the classifier in detecting the vehicles. The program applies classifier on a set of frames and calculate the percentage of vehicles identified in each frame. Next it displays the mean of the percentages. The results are as follows:\n",
    "\n",
    "* Classifier Trained with: **619 Positives and 1002 Negatives**\n",
    "* Number of Frames used: **5**\n",
    "* Mean of individual Accuracies: **17.841%**\n",
    "\n",
    "\n",
    "* Classifier Trained with: **1065 Positives and 1237 Negatives** (can be checked by right clicking 'haarTraining.bat' submitted file. Screenshots directory can be checked to see how training looks like.)\n",
    "* Number of Frames used: **5**\n",
    "* Mean of individual Accuracies:  **60.58%** (Please check the code for determining accuracy written in this notebook)\n",
    "\n",
    "\n",
    "Laplacian Threshold was changed and found to be most accurate with value 2.8 (Please check the attached threshold demonstration program)\n",
    "\n",
    "Threshold experimental result with total number of vehicles observed = 120\n",
    "\n",
    "* Threshold = 1.5  Vehicles misclassified = 54 \n",
    "\n",
    "* Threshold = 2.0  Vehicles misclassified = 39 \n",
    "\n",
    "* Threshold = 2.8  Vehicles misclassified = 19\n",
    "\n",
    "* Threshold = 3.0  Vehicles misclassified = 24\n",
    "\n",
    "* Threshold = 3.5  Vehicles misclassified = 38\n",
    "\n",
    "**Final Result with combination of threshold = 2.8 and classifier was 69% accurate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Section\n",
    "1. Program to demonstrate the working of system\n",
    "2. Program to demonstrate classifier strength\n",
    "3. Program to demonstrate  the accuracy of classifier\n",
    "4. Program to show how to mark parking spaces in a video file and store them in yaml file to feed to the system for classification\n",
    "5. Program to get an estimate of Laplacian threshold for a video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Ankit Khare\n",
    "title: Smart Parking System\n",
    "\n",
    "Instructions: Escape key to termintae the program. Please press mutiple times if it doesn't work.\n",
    "Press u to jump 500 frames and J for 1000\n",
    "Values in the dictionary can be modified:-\n",
    "1. show_ids: turn id of parking areas on or off\n",
    "2. save_video: to save the video generated by program\n",
    "3. text_overlay: displaying the frame count at the left top corner\n",
    "4. motion_detection: turn on of off, motion detection\n",
    "5. pedestrian detection: slow due to use of opencv HOG inbuilt function\n",
    "6. min_area_motion_contour: min area to take for motion tracking\n",
    "7. start_frame: from which frame number to start\n",
    "8. park_laplacian_th: set threshold vsslues for different parkings\n",
    "\"\"\"\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# path references\n",
    "fn = \"Khare_testvideo_01.mp4\" #3\n",
    "#fn = \"datasets\\parkinglot_1_720p.mp4\"\n",
    "#fn = \"datasets\\street_high_360p.mp4\"\n",
    "fn_yaml = \"Khare_yml_01.yml\"\n",
    "fn_out =  \"Khare_outputvideo_01.avi\"\n",
    "cascade_src = 'Khare_classifier_02.xml'\n",
    "car_cascade = cv2.CascadeClassifier(cascade_src)\n",
    "global_str = \"Last change at: \"\n",
    "change_pos = 0.00\n",
    "dict =  {\n",
    "        'text_overlay': True,\n",
    "        'parking_overlay': True,\n",
    "        'parking_id_overlay': True,\n",
    "        'parking_detection': True,\n",
    "        'motion_detection': True,\n",
    "        'pedestrian_detection': False, # takes a lot of processing power\n",
    "        'min_area_motion_contour': 500, # area given to detect motion\n",
    "        'park_laplacian_th': 2.8, \n",
    "        'park_sec_to_wait': 1, # 4   wait time for changing the status of a region\n",
    "        'start_frame': 0, # begin frame from specific frame number \n",
    "        'show_ids': True, # shows id on each region\n",
    "        'classifier_used': True,\n",
    "        'save_video': False\n",
    "        }\n",
    "\n",
    "# Set from video\n",
    "cap = cv2.VideoCapture(fn)\n",
    "video_info = {  'fps':    cap.get(cv2.CAP_PROP_FPS),\n",
    "                'width':  int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)*0.6),\n",
    "                'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)*0.6),\n",
    "                'fourcc': cap.get(cv2.CAP_PROP_FOURCC),\n",
    "                'num_of_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}\n",
    "\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, dict['start_frame']) # jump to frame number specified\n",
    "\n",
    "def run_classifier(img, id):\n",
    "    # gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cars = car_cascade.detectMultiScale(img, 1.1, 1)\n",
    "    if cars == ():\n",
    "        return False\n",
    "    else:\n",
    "        # parking_status[id] = False\n",
    "        return True\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "if dict['save_video']:\n",
    "    fourcc = cv2.VideoWriter_fourcc('X','V','I','D') # options: ('P','I','M','1'), ('D','I','V','X'), ('M','J','P','G'), ('X','V','I','D')\n",
    "    out = cv2.VideoWriter(fn_out, -1, 25.0,(video_info['width'], video_info['height']))\n",
    "\n",
    "# initialize the HOG descriptor/person detector. Take a lot of processing power.\n",
    "if dict['pedestrian_detection']:\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "    # Use Background subtraction\n",
    "if dict['motion_detection']:\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2(history=300, varThreshold=16, detectShadows=True)\n",
    "\n",
    "# Read YAML data (parking space polygons)\n",
    "with open(fn_yaml, 'r') as stream:\n",
    "    parking_data = yaml.load(stream)\n",
    "parking_contours = []\n",
    "parking_bounding_rects = []\n",
    "parking_mask = []\n",
    "parking_data_motion = []\n",
    "if parking_data != None:\n",
    "    for park in parking_data:\n",
    "        points = np.array(park['points'])\n",
    "        rect = cv2.boundingRect(points)\n",
    "        points_shifted = points.copy()\n",
    "        points_shifted[:,0] = points[:,0] - rect[0] # shift contour to region of interest\n",
    "        points_shifted[:,1] = points[:,1] - rect[1]\n",
    "        parking_contours.append(points)\n",
    "        parking_bounding_rects.append(rect)\n",
    "        mask = cv2.drawContours(np.zeros((rect[3], rect[2]), dtype=np.uint8), [points_shifted], contourIdx=-1,\n",
    "                                    color=255, thickness=-1, lineType=cv2.LINE_8)\n",
    "        mask = mask==255\n",
    "        parking_mask.append(mask)\n",
    "\n",
    "kernel_erode = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)) # morphological kernel\n",
    "kernel_dilate = cv2.getStructuringElement(cv2.MORPH_RECT,(5,19))\n",
    "if parking_data != None:\n",
    "    parking_status = [False]*len(parking_data)\n",
    "    parking_buffer = [None]*len(parking_data)\n",
    "# bw = ()\n",
    "def print_parkIDs(park, coor_points, frame_rev):\n",
    "    moments = cv2.moments(coor_points)\n",
    "    centroid = (int(moments['m10']/moments['m00'])-3, int(moments['m01']/moments['m00'])+3)\n",
    "    # putting numbers on marked regions\n",
    "    cv2.putText(frame_rev, str(park['id']), (centroid[0]+1, centroid[1]+1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame_rev, str(park['id']), (centroid[0]-1, centroid[1]-1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame_rev, str(park['id']), (centroid[0]+1, centroid[1]-1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame_rev, str(park['id']), (centroid[0]-1, centroid[1]+1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame_rev, str(park['id']), centroid, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "    \n",
    "while(cap.isOpened()):\n",
    "    video_cur_pos = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0 # Current position of the video file in seconds\n",
    "    video_cur_frame = cap.get(cv2.CAP_PROP_POS_FRAMES) # Index of the frame to be decoded/captured next\n",
    "    ret, frame_initial = cap.read()\n",
    "    if ret == True:\n",
    "        frame = cv2.resize(frame_initial, None, fx=0.6, fy=0.6)\n",
    "    if ret == False:\n",
    "        print(\"Video ended\")\n",
    "        break\n",
    "\n",
    "    # Background Subtraction\n",
    "    frame_blur = cv2.GaussianBlur(frame.copy(), (5,5), 3)\n",
    "    # frame_blur = frame_blur[150:1000, 100:1800]\n",
    "    frame_gray = cv2.cvtColor(frame_blur, cv2.COLOR_BGR2GRAY)\n",
    "    frame_out = frame.copy()\n",
    "\n",
    "    # Drawing the Overlay. Text overlay at the left corner of screen\n",
    "    if dict['text_overlay']:\n",
    "        str_on_frame = \"%d/%d\" % (video_cur_frame, video_info['num_of_frames'])\n",
    "        cv2.putText(frame_out, str_on_frame, (5,30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.8, (0,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame_out,global_str + str(round(change_pos,2)) + 'sec', (5, 60), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # motion detection for all objects\n",
    "    if dict['motion_detection']:\n",
    "        # frame_blur = frame_blur[380:420, 240:470]\n",
    "        # cv2.imshow('dss', frame_blur)\n",
    "        fgmask = fgbg.apply(frame_blur)\n",
    "        bw = np.uint8(fgmask==255)*255\n",
    "        bw = cv2.erode(bw, kernel_erode, iterations=1)\n",
    "        bw = cv2.dilate(bw, kernel_dilate, iterations=1)\n",
    "        # cv2.imshow('dss',bw)\n",
    "        # cv2.imwrite(\"frame%d.jpg\" % co, bw)\n",
    "        (_, cnts, _) = cv2.findContours(bw.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # loop over the contours\n",
    "        for c in cnts:\n",
    "            # print(cv2.contourArea(c))\n",
    "            # if the contour is too small, we ignore it\n",
    "            if cv2.contourArea(c) < dict['min_area_motion_contour']:\n",
    "                continue\n",
    "            (x, y, w, h) = cv2.boundingRect(c)\n",
    "            cv2.rectangle(frame_out, (x, y), (x + w, y + h), (255, 0, 0), 1)\n",
    "\n",
    "    # detecting cars and vacant spaces\n",
    "    if dict['parking_detection']:\n",
    "        for ind, park in enumerate(parking_data):\n",
    "            points = np.array(park['points'])\n",
    "            rect = parking_bounding_rects[ind]\n",
    "            roi_gray = frame_gray[rect[1]:(rect[1]+rect[3]), rect[0]:(rect[0]+rect[2])] # crop roi for faster calcluation\n",
    "\n",
    "            laplacian = cv2.Laplacian(roi_gray, cv2.CV_64F)\n",
    "            # cv2.imshow('oir', laplacian)\n",
    "            points[:,0] = points[:,0] - rect[0] # shift contour to roi\n",
    "            points[:,1] = points[:,1] - rect[1]\n",
    "            delta = np.mean(np.abs(laplacian * parking_mask[ind]))\n",
    "            # if(delta<2.5):\n",
    "                # print(\"ind, del\", ind, delta)\n",
    "            status = delta < dict['park_laplacian_th']\n",
    "            # If detected a change in parking status, save the current time\n",
    "            if status != parking_status[ind] and parking_buffer[ind]==None:\n",
    "                parking_buffer[ind] = video_cur_pos\n",
    "                change_pos = video_cur_pos\n",
    "                # print(\"state \", ind,delta)\n",
    "                # applying classifier in case a change is detected in the status of area\n",
    "                # if dict['classifier_used']:\n",
    "                #     classifier_result = run_classifier(roi_gray)\n",
    "                #     if classifier_result:\n",
    "                #         print(classifier_result)\n",
    "            # If status is still different than the one saved and counter is open\n",
    "            elif status != parking_status[ind] and parking_buffer[ind]!=None:\n",
    "                if video_cur_pos - parking_buffer[ind] > dict['park_sec_to_wait']:\n",
    "                    parking_status[ind] = status\n",
    "                    parking_buffer[ind] = None\n",
    "            # If status is still same and counter is open\n",
    "            elif status == parking_status[ind] and parking_buffer[ind]!=None:\n",
    "                parking_buffer[ind] = None\n",
    "\n",
    "    # changing the color on the basis on status change occured in the above section and putting numbers on areas\n",
    "    if dict['parking_overlay']:\n",
    "        for ind, park in enumerate(parking_data):\n",
    "            points = np.array(park['points'])\n",
    "            if parking_status[ind]:\n",
    "                color = (0,255,0)\n",
    "                rect = parking_bounding_rects[ind]\n",
    "                roi_gray_ov = frame_gray[rect[1]:(rect[1] + rect[3]),\n",
    "                               rect[0]:(rect[0] + rect[2])]  # crop roi for faster calcluation\n",
    "                res = run_classifier(roi_gray_ov, ind)\n",
    "                if res:\n",
    "                    parking_data_motion.append(parking_data[ind])\n",
    "                    # del parking_data[ind]\n",
    "                    color = (0,0,255)\n",
    "            else:\n",
    "                color = (0,0,255)\n",
    "            \n",
    "            cv2.drawContours(frame_out, [points], contourIdx=-1,\n",
    "                                 color=color, thickness=2, lineType=cv2.LINE_8)\n",
    "            if dict['show_ids']:\n",
    "                    print_parkIDs(park, points, frame_out)\n",
    "            \n",
    "            \n",
    "\n",
    "    if parking_data_motion != []:\n",
    "        for index, park_coord in enumerate(parking_data_motion):\n",
    "            points = np.array(park_coord['points'])\n",
    "            color = (0, 0, 255)\n",
    "            recta = parking_bounding_rects[ind]\n",
    "            roi_gray1 = frame_gray[recta[1]:(recta[1] + recta[3]),\n",
    "                            recta[0]:(recta[0] + recta[2])]  # crop roi for faster calcluation\n",
    "            # laplacian = cv2.Laplacian(roi_gray, cv2.CV_64F)\n",
    "            # delta2 = np.mean(np.abs(laplacian * parking_mask[ind]))\n",
    "            # state = delta2<1\n",
    "            # classifier_result = run_classifier(roi_gray1, index)\n",
    "            # cv2.imshow('dsd', roi_gray1)\n",
    "            fgbg1 = cv2.createBackgroundSubtractorMOG2(history=300, varThreshold=16, detectShadows=True)\n",
    "            roi_gray1_blur = cv2.GaussianBlur(roi_gray1.copy(), (5, 5), 3)\n",
    "            # cv2.imshow('sd', roi_gray1_blur)\n",
    "            fgmask1 = fgbg1.apply(roi_gray1_blur)\n",
    "            bw1 = np.uint8(fgmask1 == 255) * 255\n",
    "            bw1 = cv2.erode(bw1, kernel_erode, iterations=1)\n",
    "            bw1 = cv2.dilate(bw1, kernel_dilate, iterations=1)\n",
    "            # cv2.imshow('sd', bw1)\n",
    "            # cv2.imwrite(\"frame%d.jpg\" % co, bw)\n",
    "            (_, cnts1, _) = cv2.findContours(bw1.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            # loop over the contours\n",
    "            for c in cnts1:\n",
    "                print(cv2.contourArea(c))\n",
    "                # if the contour is too small, we ignore it\n",
    "                if cv2.contourArea(c) < 4:\n",
    "                    continue\n",
    "                (x, y, w, h) = cv2.boundingRect(c)\n",
    "                classifier_result1 = run_classifier(roi_gray1, index)\n",
    "                if classifier_result1:\n",
    "                # print(classifier_result)\n",
    "                    color = (0, 0, 255)  # Red again if car found by classifier\n",
    "                else:\n",
    "                    color = (0,255, 0)\n",
    "            classifier_result1 = run_classifier(roi_gray1, index)\n",
    "            if classifier_result1:\n",
    "                # print(classifier_result)\n",
    "                color = (0, 0, 255)  # Red again if car found by classifier\n",
    "            else:\n",
    "                color = (0, 255, 0)\n",
    "            cv2.drawContours(frame_out, [points], contourIdx=-1,\n",
    "                                 color=color, thickness=2, lineType=cv2.LINE_8)\n",
    "\n",
    "    if dict['pedestrian_detection']:\n",
    "        # detect people in the image. Slows down the program, requires high GPU speed\n",
    "        (rects, weights) = hog.detectMultiScale(frame, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "        # draw the  bounding boxes\n",
    "        for (x, y, w, h) in rects:\n",
    "            cv2.rectangle(frame_out, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    # write the output frames\n",
    "    if dict['save_video']:\n",
    "#         if video_cur_frame % 35 == 0: # take every 30 frames\n",
    "            out.write(frame_out)\n",
    "\n",
    "    # Display video\n",
    "    cv2.imshow('frame', frame_out)\n",
    "    # cv2.imshow('background mask', bw)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "    elif k == ord('c'):\n",
    "        cv2.imwrite('frame%d.jpg' % video_cur_frame, frame_out)\n",
    "    elif k == ord('j'):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, video_cur_frame+1000) # jump 1000 frames\n",
    "    elif k == ord('u'):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, video_cur_frame + 500)  # jump 500 frames\n",
    "    if cv2.waitKey(33) == 27:\n",
    "        break\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cap.release()\n",
    "if dict['save_video']: out.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.6.1\n",
      "alabaster==0.7.10\n",
      "anaconda-client==1.6.9\n",
      "anaconda-navigator==1.7.0\n",
      "anaconda-project==0.8.2\n",
      "asgiref==3.2.3\n",
      "asn1crypto==0.24.0\n",
      "astor==0.7.1\n",
      "astroid==1.6.1\n",
      "astropy==2.0.3\n",
      "attrs==17.4.0\n",
      "Babel==2.5.3\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "beautifulsoup4==4.6.0\n",
      "bitarray==0.8.1\n",
      "bkcharts==0.2\n",
      "blaze==0.11.3\n",
      "bleach==2.1.2\n",
      "bokeh==0.12.13\n",
      "boto==2.48.0\n",
      "Bottleneck==1.2.1\n",
      "branca==0.3.1\n",
      "camera==1.3.0\n",
      "certifi==2019.3.9\n",
      "cffi==1.11.4\n",
      "chardet==3.0.4\n",
      "click==6.7\n",
      "cloudpickle==0.5.2\n",
      "clyent==1.2.2\n",
      "cmake==3.13.3\n",
      "colorama==0.3.9\n",
      "comtypes==1.1.4\n",
      "conda==4.6.14\n",
      "conda-build==3.4.1\n",
      "conda-verify==2.0.0\n",
      "config==0.4.2\n",
      "contextlib2==0.5.5\n",
      "cryptography==2.6.1\n",
      "CVPubSubs==0.6.4\n",
      "cycler==0.10.0\n",
      "cymem==2.0.2\n",
      "Cython==0.27.3\n",
      "cytoolz==0.9.0\n",
      "dask==0.16.1\n",
      "datashape==0.5.4\n",
      "decorator==4.2.1\n",
      "distributed==1.20.2\n",
      "Django==3.0.2\n",
      "dlib==19.7.0\n",
      "docutils==0.14\n",
      "entrypoints==0.2.3\n",
      "et-xmlfile==1.0.1\n",
      "face-recognition==1.2.3\n",
      "face-recognition-models==0.3.0\n",
      "fastcache==1.0.2\n",
      "filelock==2.0.13\n",
      "Flask==0.12.2\n",
      "Flask-Cors==3.0.3\n",
      "Flask-MySQL==1.4.0\n",
      "Flask-MySQLdb==0.2.0\n",
      "Flask-WTF==0.14.2\n",
      "folium==0.8.3\n",
      "gast==0.2.0\n",
      "gevent==1.2.2\n",
      "glob2==0.6\n",
      "gmplot==1.2.0\n",
      "graphviz==0.8.4\n",
      "greenlet==0.4.12\n",
      "grpcio==1.17.1\n",
      "h5py==2.7.1\n",
      "heapdict==1.0.0\n",
      "html5lib==1.0.1\n",
      "idna==2.6\n",
      "imageio==2.2.0\n",
      "imagesize==0.7.1\n",
      "imutils==0.5.2\n",
      "infinity==1.4\n",
      "intervals==0.8.1\n",
      "ipykernel==5.1.3\n",
      "ipython==6.2.1\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.1.1\n",
      "isort==4.2.15\n",
      "itsdangerous==0.24\n",
      "jdcal==1.3\n",
      "jedi==0.11.1\n",
      "Jinja2==2.10\n",
      "jsonschema==2.6.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.2.2\n",
      "jupyter-console==5.2.0\n",
      "jupyter-core==4.4.0\n",
      "jupyterlab==0.31.4\n",
      "jupyterlab-launcher==0.10.2\n",
      "Keras==2.2.4\n",
      "Keras-Applications==1.0.6\n",
      "Keras-Preprocessing==1.0.5\n",
      "kiwisolver==1.1.0\n",
      "lazy-object-proxy==1.3.1\n",
      "llvmlite==0.21.0\n",
      "localpubsub==0.0.4\n",
      "locket==0.2.0\n",
      "lxml==4.1.1\n",
      "Markdown==3.0.1\n",
      "MarkupSafe==1.0\n",
      "mathparse==0.1.2\n",
      "matplotlib==2.2.2\n",
      "mccabe==0.6.1\n",
      "menuinst==1.4.11\n",
      "mistune==0.8.3\n",
      "mock==4.0.1\n",
      "mpmath==1.0.0\n",
      "msgpack-python==0.5.1\n",
      "multipledispatch==0.4.9\n",
      "murmurhash==1.0.2\n",
      "mxnet==1.4.1\n",
      "mysqlclient==1.4.2.post1\n",
      "navigator-updater==0.1.0\n",
      "nbconvert==5.3.1\n",
      "nbformat==4.4.0\n",
      "networkx==2.1\n",
      "nltk==3.2.5\n",
      "nose==1.3.7\n",
      "notebook==5.4.0\n",
      "numba==0.36.2\n",
      "numexpr==2.6.4\n",
      "numpy==1.16.1\n",
      "numpydoc==0.7.0\n",
      "odo==0.5.1\n",
      "olefile==0.45.1\n",
      "opencv-contrib-python==4.1.1.26\n",
      "opencv-python==3.4.5.20\n",
      "openpyxl==2.4.10\n",
      "packaging==16.8\n",
      "panda==0.3.1\n",
      "pandas==0.22.0\n",
      "pandas-datareader==0.7.0\n",
      "pandocfilters==1.4.2\n",
      "parso==0.1.1\n",
      "partd==0.3.8\n",
      "path.py==10.5\n",
      "pathlib2==2.3.0\n",
      "patsy==0.5.0\n",
      "pep8==1.7.1\n",
      "pickleshare==0.7.4\n",
      "Pillow==5.0.0\n",
      "pkginfo==1.4.1\n",
      "plac==0.9.6\n",
      "playsound==1.2.2\n",
      "pluggy==0.6.0\n",
      "ply==3.10\n",
      "preshed==2.0.1\n",
      "progress==1.5\n",
      "prompt-toolkit==1.0.15\n",
      "protobuf==3.6.1\n",
      "psutil==5.4.3\n",
      "pubnub==4.1.3\n",
      "py==1.5.2\n",
      "pycodestyle==2.3.1\n",
      "pycosat==0.6.3\n",
      "pycparser==2.18\n",
      "pycrypto==2.6.1\n",
      "pycryptodomex==3.8.1\n",
      "pycurl==7.43.0.2\n",
      "pydot==1.4.1\n",
      "pydotplus==2.0.2\n",
      "pyflakes==1.6.0\n",
      "Pygments==2.2.0\n",
      "pylint==1.8.2\n",
      "PyMySQL==0.9.3\n",
      "pyodbc==4.0.22\n",
      "pyOpenSSL==17.5.0\n",
      "pyparsing==2.2.0\n",
      "pypi==2.1\n",
      "pyserial==3.4\n",
      "PySocks==1.6.7\n",
      "pytest==3.3.2\n",
      "python-dateutil==2.7.5\n",
      "pytils==0.3\n",
      "pytz==2017.3\n",
      "PyWavelets==0.5.2\n",
      "pywin32==222\n",
      "pywinpty==0.5\n",
      "PyYAML==3.12\n",
      "pyzmq==16.0.3\n",
      "QtAwesome==0.4.4\n",
      "qtconsole==4.3.1\n",
      "QtPy==1.3.1\n",
      "requests==2.18.4\n",
      "rope==0.10.7\n",
      "ruamel-yaml==0.15.35\n",
      "scikit-image==0.13.1\n",
      "scikit-learn==0.19.1\n",
      "scipy==1.0.0\n",
      "seaborn==0.8.1\n",
      "selenium==3.141.0\n",
      "Send2Trash==1.4.2\n",
      "simplegeneric==0.8.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.11.0\n",
      "snowballstemmer==1.2.1\n",
      "sortedcollections==0.5.3\n",
      "sortedcontainers==1.5.9\n",
      "Sphinx==1.6.6\n",
      "sphinxcontrib-websupport==1.0.1\n",
      "spyder==3.2.6\n",
      "SQLAlchemy==1.2.1\n",
      "sqlparse==0.3.0\n",
      "srsly==0.0.5\n",
      "statsmodels==0.8.0\n",
      "sympy==1.1.1\n",
      "tables==3.4.2\n",
      "tblib==1.3.2\n",
      "tensorboard==1.13.1\n",
      "tensorflow==1.13.1\n",
      "tensorflow-estimator==1.13.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.1\n",
      "testpath==0.3.1\n",
      "tools==0.1.9\n",
      "toolz==0.9.0\n",
      "tornado==4.5.3\n",
      "tqdm==4.41.1\n",
      "traitlets==4.3.2\n",
      "typing==3.6.2\n",
      "unicodecsv==0.14.1\n",
      "urllib3==1.22\n",
      "validators==0.14.0\n",
      "vidstab==1.7.2\n",
      "virtualenv==16.7.9\n",
      "virtualenvwrapper-win==1.2.5\n",
      "wasabi==0.2.2\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.14.1\n",
      "widgetsnbextension==3.1.0\n",
      "win-inet-pton==1.0.1\n",
      "win-unicode-console==0.5\n",
      "wincertstore==0.2\n",
      "wrapt==1.10.11\n",
      "WTForms==2.2.1\n",
      "WTForms-Components==0.10.4\n",
      "xlrd==1.1.0\n",
      "XlsxWriter==1.0.2\n",
      "xlwings==0.11.5\n",
      "xlwt==1.3.0\n",
      "zict==0.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could not generate requirement for distribution -umpy 1.14.2 (c:\\users\\public\\anaconda3\\lib\\site-packages): Parse error at \"'-umpy==1'\": Expected W:(abcd...)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
